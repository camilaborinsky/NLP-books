{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Paragraph\n",
      "Author                                     \n",
      "Alcott, Louisa May                      103\n",
      "Austen, Jane                            103\n",
      "Brontë, Charlotte                       103\n",
      "Christie, Agatha                        103\n",
      "Dickens, Charles                        103\n",
      "Dostoyevsky, Fyodor                     103\n",
      "Doyle, Arthur Conan                     103\n",
      "Dumas, Alexandre                        103\n",
      "Hugo, Victor                            103\n",
      "Marcus Aurelius, Emperor of Rome        103\n",
      "Nietzsche, Friedrich Wilhelm            103\n",
      "Poe, Edgar Allan                        103\n",
      "Shakespeare, William                    103\n",
      "Twain, Mark                             103\n",
      "Verne, Jules                            103\n",
      "Wells, H. G. (Herbert George)           103\n",
      "Wilde, Oscar                            103\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/gutenberg_paragraphs.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Stay with only 10% of the min count of paragraphs per author\n",
    "min_count = df.groupby('Author').count().min()[0]\n",
    "df = df.groupby('Author').head(int(min_count*0.10))\n",
    "print(df.groupby('Author').count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "authors = df['Author'].unique()\n",
    "author2idx = {author: idx for idx, author in enumerate(authors)}\n",
    "idx2author = {idx: author for idx, author in enumerate(authors)}\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.labels = [author2idx[author] for author in df['Author']]\n",
    "        print(self.labels)\n",
    "        self.texts = [ tokenizer(paragraph, padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for paragraph in df['Paragraph']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, len(authors))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    #use_cuda = False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    #use_cuda = False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 175 176\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 4, 2, 14, 15, 15, 0, 11, 15, 7, 5, 7, 0, 12, 12, 5, 7, 10, 11, 3, 7, 8, 2, 0, 9, 9, 15, 5, 9, 10, 2, 16, 4, 11, 14, 5, 5, 10, 3, 0, 3, 3, 1, 10, 1, 11, 9, 0, 6, 14, 13, 7, 8, 16, 1, 10, 15, 15, 3, 3, 6, 2, 15, 1, 2, 7, 12, 16, 7, 8, 2, 4, 13, 11, 2, 11, 12, 16, 10, 14, 16, 9, 7, 8, 14, 4, 13, 9, 11, 15, 10, 11, 11, 12, 15, 1, 16, 4, 16, 10, 9, 14, 3, 0, 13, 9, 1, 10, 7, 3, 7, 15, 3, 3, 4, 3, 5, 5, 7, 10, 6, 6, 2, 14, 6, 15, 6, 13, 11, 15, 0, 7, 2, 15, 14, 2, 6, 10, 6, 10, 10, 6, 13, 6, 1, 0, 14, 3, 8, 9, 3, 14, 10, 14, 16, 1, 0, 14, 6, 9, 15, 5, 13, 16, 14, 6, 2, 7, 14, 6, 4, 7, 10, 8, 13, 3, 7, 16, 12, 11, 8, 3, 11, 5, 2, 8, 0, 7, 4, 16, 4, 11, 14, 15, 2, 8, 13, 9, 6, 0, 11, 6, 0, 15, 15, 4, 7, 14, 1, 4, 14, 1, 16, 5, 7, 5, 11, 6, 4, 14, 12, 13, 15, 0, 14, 7, 10, 12, 6, 10, 15, 3, 3, 2, 16, 14, 10, 4, 5, 2, 8, 15, 5, 3, 3, 12, 3, 7, 16, 12, 11, 7, 16, 10, 9, 0, 10, 6, 4, 15, 12, 9, 0, 14, 2, 6, 9, 5, 9, 0, 14, 0, 2, 3, 5, 8, 10, 8, 3, 2, 13, 4, 6, 16, 16, 12, 9, 11, 16, 12, 3, 4, 7, 2, 1, 0, 14, 16, 8, 14, 12, 6, 10, 14, 3, 11, 3, 6, 4, 13, 9, 5, 13, 2, 15, 1, 16, 7, 15, 9, 13, 9, 4, 10, 1, 1, 15, 14, 8, 16, 0, 4, 15, 9, 9, 7, 11, 1, 5, 6, 4, 1, 7, 8, 7, 16, 9, 5, 11, 0, 8, 14, 7, 15, 16, 13, 1, 9, 8, 2, 5, 13, 6, 5, 16, 9, 16, 2, 7, 4, 9, 6, 2, 7, 0, 15, 15, 9, 2, 4, 8, 2, 8, 14, 3, 8, 9, 8, 4, 13, 1, 11, 12, 13, 6, 4, 16, 1, 13, 8, 16, 12, 16, 1, 8, 14, 9, 9, 3, 2, 9, 12, 14, 3, 1, 0, 6, 0, 6, 4, 12, 13, 14, 4, 8, 8, 8, 10, 6, 2, 15, 8, 10, 9, 14, 13, 11, 8, 16, 1, 2, 6, 3, 5, 5, 13, 14, 5, 4, 7, 1, 12, 12, 4, 15, 15, 15, 14, 15, 16, 11, 5, 14, 8, 4, 15, 16, 8, 4, 10, 16, 12, 13, 11, 6, 1, 12, 8, 9, 8, 1, 12, 12, 15, 16, 8, 11, 4, 0, 1, 2, 7, 11, 16, 7, 16, 11, 13, 16, 4, 7, 9, 0, 16, 15, 6, 11, 0, 5, 11, 14, 16, 15, 8, 15, 3, 5, 2, 0, 14, 7, 11, 15, 2, 6, 15, 9, 16, 3, 5, 12, 6, 14, 15, 8, 4, 16, 15, 9, 1, 13, 7, 12, 2, 2, 10, 14, 5, 5, 2, 5, 16, 13, 4, 3, 11, 12, 7, 0, 15, 8, 5, 4, 13, 9, 5, 8, 5, 1, 3, 14, 10, 2, 1, 11, 16, 4, 6, 8, 0, 6, 9, 3, 4, 7, 8, 10, 14, 2, 9, 9, 12, 14, 6, 6, 12, 4, 8, 6, 6, 8, 15, 12, 12, 14, 3, 0, 4, 8, 13, 10, 4, 16, 14, 7, 2, 5, 16, 14, 10, 16, 4, 4, 13, 0, 16, 7, 12, 3, 15, 9, 6, 0, 0, 5, 12, 13, 3, 13, 4, 0, 6, 15, 5, 14, 14, 10, 11, 11, 7, 10, 13, 8, 6, 10, 2, 8, 3, 0, 6, 5, 6, 9, 16, 16, 13, 15, 2, 11, 12, 5, 7, 4, 2, 10, 9, 5, 1, 10, 2, 2, 16, 10, 11, 5, 1, 9, 1, 8, 0, 10, 11, 6, 0, 2, 9, 4, 1, 15, 2, 1, 8, 0, 13, 15, 0, 3, 11, 10, 12, 7, 13, 12, 6, 5, 9, 13, 3, 7, 15, 3, 12, 16, 10, 12, 11, 6, 11, 9, 13, 15, 4, 5, 12, 16, 1, 1, 1, 2, 14, 6, 2, 9, 13, 16, 16, 2, 12, 7, 5, 11, 0, 15, 12, 12, 3, 4, 3, 13, 6, 1, 11, 4, 5, 14, 3, 5, 11, 11, 12, 11, 8, 8, 14, 10, 1, 3, 4, 0, 7, 9, 1, 16, 9, 2, 13, 4, 6, 10, 2, 2, 6, 10, 13, 1, 10, 8, 4, 8, 3, 6, 5, 14, 4, 2, 14, 8, 3, 13, 15, 10, 3, 16, 14, 12, 13, 5, 13, 15, 7, 3, 16, 3, 1, 7, 3, 1, 0, 6, 10, 8, 12, 6, 9, 7, 15, 6, 14, 1, 11, 1, 7, 0, 13, 0, 14, 2, 7, 3, 3, 5, 4, 13, 6, 0, 4, 14, 8, 3, 12, 6, 15, 10, 4, 12, 2, 8, 12, 10, 0, 7, 6, 10, 13, 4, 16, 10, 2, 2, 6, 14, 8, 4, 5, 15, 3, 11, 6, 14, 6, 12, 1, 3, 12, 0, 6, 11, 12, 9, 4, 12, 11, 4, 0, 7, 3, 11, 5, 11, 8, 5, 5, 1, 8, 14, 7, 7, 2, 14, 4, 8, 0, 11, 11, 7, 6, 13, 12, 16, 2, 1, 1, 9, 6, 11, 7, 15, 10, 10, 2, 10, 14, 1, 0, 15, 6, 10, 6, 12, 6, 16, 2, 10, 9, 12, 0, 5, 11, 10, 5, 10, 16, 4, 0, 10, 11, 13, 1, 11, 5, 3, 10, 9, 11, 2, 10, 10, 3, 9, 7, 9, 11, 14, 7, 13, 2, 1, 0, 3, 4, 6, 5, 3, 9, 8, 6, 15, 4, 3, 7, 15, 0, 13, 11, 4, 3, 3, 12, 13, 8, 12, 6, 13, 2, 5, 13, 12, 11, 1, 8, 4, 1, 1, 7, 6, 13, 6, 12, 4, 9, 16, 1, 1, 1, 11, 6, 6, 14, 1, 0, 11, 4, 13, 10, 10, 7, 16, 1, 12, 1, 3, 13, 0, 3, 9, 13, 8, 10, 1, 14, 3, 5, 8, 15, 12, 13, 14, 13, 4, 0, 3, 3, 8, 5, 8, 0, 11, 15, 11, 14, 0, 5, 7, 13, 13, 2, 4, 6, 9, 1, 9, 5, 1, 1, 5, 12, 2, 7, 1, 0, 15, 13, 14, 16, 16, 8, 5, 1, 14, 6, 14, 7, 6, 14, 11, 0, 4, 16, 2, 6, 8, 1, 4, 16, 0, 1, 16, 0, 5, 5, 6, 0, 13, 4, 0, 0, 16, 15, 6, 4, 4, 8, 13, 0, 15, 2, 5, 13, 9, 8, 10, 7, 3, 4, 3, 13, 14, 15, 10, 3, 13, 9, 11, 10, 14, 0, 15, 10, 4, 13, 12, 3, 7, 5, 10, 11, 7, 2, 6, 15, 13, 9, 1, 16, 10, 1, 14, 13, 5, 11, 7, 16, 12, 11, 11, 3, 7, 9, 5, 7, 2, 12, 3, 4, 0, 2, 6, 2, 10, 16, 8, 14, 13, 9, 11, 11, 16, 2, 4, 5, 7, 9, 1, 1, 2, 13, 13, 1, 2, 3, 4, 12, 5, 11, 15, 0, 0, 11, 9, 16, 9, 8, 12, 15, 10, 10, 9, 11, 8, 6, 3, 4, 2, 15, 16, 9, 16, 5, 8, 9, 9, 12, 6, 15, 15, 0, 10, 13, 15, 0, 5, 0, 14, 14, 15, 2, 11, 10, 0, 16, 11, 7, 1, 12, 8, 7, 10, 2, 14, 7, 11, 11, 16, 7, 4, 9, 3, 2, 12, 0, 4, 11, 14, 10, 8, 9, 15, 3, 10, 13, 6, 12, 12, 2, 0, 0, 2, 12, 5, 16, 2, 5, 8, 1, 9, 5, 15, 8, 10, 12, 9, 8, 8, 2, 5, 7, 5, 4, 2, 5, 4, 8, 13, 2, 2, 14, 8, 8, 8, 10, 0, 8, 6, 4, 0, 1, 12, 8, 4, 16, 5, 13, 2, 2, 14, 1, 15, 4, 0, 15, 5, 9, 11, 15, 13, 6, 5, 5, 4, 14, 11, 13, 15, 9, 8, 3, 7, 13, 14, 8, 16, 15, 6, 4, 13, 11, 12, 10, 12]\n",
      "[3, 8, 11, 4, 3, 16, 2, 16, 15, 4, 12, 14, 1, 1, 2, 14, 15, 2, 13, 0, 11, 10, 9, 13, 11, 14, 16, 7, 0, 15, 10, 8, 12, 2, 4, 5, 16, 4, 15, 16, 1, 3, 7, 12, 0, 3, 2, 9, 15, 5, 16, 0, 13, 5, 11, 13, 1, 14, 8, 11, 9, 12, 8, 13, 7, 4, 14, 3, 1, 1, 7, 7, 2, 15, 12, 7, 0, 5, 16, 0, 3, 13, 6, 0, 15, 0, 15, 6, 16, 5, 16, 1, 4, 9, 7, 9, 3, 9, 7, 3, 13, 12, 8, 10, 3, 11, 2, 5, 14, 13, 16, 10, 14, 4, 16, 10, 10, 4, 1, 9, 4, 13, 9, 12, 1, 14, 13, 13, 5, 13, 15, 12, 12, 12, 6, 15, 10, 12, 11, 2, 12, 12, 6, 14, 14, 0, 8, 7, 11, 14, 1, 16, 15, 10, 14, 12, 16, 1, 16, 16, 0, 9, 8, 7, 10, 5, 9, 16, 10, 7, 0, 8, 9, 4, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 116/700 [00:17<01:27,  6.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m BertClassifier()\n\u001b[1;32m      3\u001b[0m LR \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n\u001b[0;32m----> 5\u001b[0m train(model, df_train, df_val, LR, EPOCHS)\n",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m output \u001b[39m=\u001b[39m model(input_id, mask)\n\u001b[1;32m     33\u001b[0m batch_loss \u001b[39m=\u001b[39m criterion(output, train_label\u001b[39m.\u001b[39mlong())\n\u001b[0;32m---> 34\u001b[0m total_loss_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     36\u001b[0m acc \u001b[39m=\u001b[39m (output\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m train_label)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m total_acc_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), 'model2.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
