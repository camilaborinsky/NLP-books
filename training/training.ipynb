{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/app/data/gutenberg_paragraphs.csv')\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#count paragraphs per author\n",
    "df.groupby('Author').count()\n",
    "\n",
    "#select only X paragraphs per author\n",
    "df = df.groupby('Author').head(20).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "authors = df['Author'].unique()\n",
    "author2idx = {author: idx for idx, author in enumerate(authors)}\n",
    "idx2author = {idx: author for idx, author in enumerate(authors)}\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.labels = [author2idx[author] for author in df['Author']]\n",
    "        print(self.labels)\n",
    "        self.texts = [ tokenizer(paragraph, padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for paragraph in df['Paragraph']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.linear = nn.Linear(768, len(authors))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        linear_output = self.linear(pooled_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    #use_cuda = False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {total_acc_train / len(train_data): .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    #use_cuda = False\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480 60 60\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "                                     [int(.8*len(df)), int(.9*len(df))])\n",
    "\n",
    "print(len(df_train),len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing BertModel: ['distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'vocab_layer_norm.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'vocab_transform.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.embeddings.word_embeddings.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 1, 3, 2, 0, 1, 4, 2, 0, 3, 2, 5, 1, 4, 0, 1, 0, 4, 0, 2, 1, 3, 1, 4, 3, 4, 0, 1, 2, 1, 2, 5, 3, 0, 4, 3, 1, 1, 4, 5, 5, 1, 1, 0, 0, 2, 5, 0, 1, 3, 0, 4, 1, 1, 0, 3, 0, 3, 4, 5, 2, 0, 3, 3, 2, 1, 1, 0, 0, 4, 1, 5, 2, 4, 1, 0, 5, 1, 4, 5, 5, 1, 2, 5, 0, 1, 5, 0, 1, 5, 3, 5, 5, 0, 0, 5, 4, 2, 5, 0, 2, 1, 5, 0, 2, 5, 0, 1, 1, 0, 2, 1, 1, 3, 4, 2, 3, 3, 4, 5, 0, 5, 3, 4, 5, 1, 4, 1, 2, 3, 1, 5, 5, 2, 5, 5, 2, 4, 5, 2, 0, 2, 0, 4, 4, 5, 5, 4, 0, 1, 3, 5, 5, 4, 3, 4, 2, 4, 5, 2, 0, 3, 0, 0, 1, 5, 0, 1, 5, 1, 4, 3, 5, 4, 3, 4, 4, 5, 2, 4, 2, 1, 3, 2, 5, 1, 2, 3, 0, 2, 3, 5, 1, 2, 2, 4, 5, 3, 0, 0, 5, 3, 3, 2, 2, 2, 4, 2, 2, 3, 3, 3, 4, 0, 1, 0, 3, 4, 4, 2, 2, 2, 4, 3, 3, 4, 5, 4, 0, 2, 3, 5, 4, 3, 1, 0, 4, 1, 5, 1, 5, 5, 3, 4, 3, 3, 4, 0, 2, 3, 2, 0, 1, 2, 0, 5, 0, 4, 2, 2, 5, 1, 2, 3, 3, 1, 2, 3, 2, 0, 5, 2, 5, 2, 1, 1, 0, 1, 1, 3, 0, 4, 3, 0, 3, 0, 1, 0, 2, 1, 4, 2, 4, 5, 3, 1, 5, 3, 1, 0, 3, 1, 3, 2, 2, 2, 0, 3, 2, 5, 3, 2, 1, 2, 5, 2, 2, 4, 1, 2, 2, 5, 2, 0, 4, 5, 0, 2, 4, 0, 4, 0, 4, 0, 3, 1, 5, 2, 3, 5, 3, 4, 1, 0, 4, 4, 0, 5, 1, 3, 0, 2, 2, 4, 3, 5, 5, 4, 4, 5, 4, 5, 5, 4, 5, 5, 5, 2, 1, 2, 2, 4, 5, 4, 5, 4, 4, 1, 3, 3, 5, 4, 1, 3, 4, 2, 1, 5, 1, 2, 4, 1, 5, 1, 0, 3, 1, 0, 3, 2, 0, 5, 0, 0, 3, 1, 3, 3, 0, 1, 0, 2, 3, 0, 4, 2, 4, 5, 1, 3, 4, 0, 3, 2, 5, 3, 2, 1, 2, 2, 0, 1, 0, 3, 5, 4, 1, 4, 0, 3, 0, 0, 0, 5, 2, 1, 2, 5, 0, 5, 5, 1, 5, 5, 4, 4, 3, 2, 3, 3, 2, 3, 4, 0, 3, 4, 1, 4, 0, 3, 4, 2, 2, 2, 5, 4, 0, 4, 0]\n",
      "[0, 1, 3, 2, 3, 5, 4, 3, 1, 4, 3, 0, 4, 4, 5, 2, 4, 4, 5, 5, 1, 3, 0, 5, 5, 1, 4, 2, 2, 4, 0, 2, 5, 3, 1, 5, 3, 1, 3, 1, 4, 1, 0, 1, 3, 3, 5, 0, 3, 0, 1, 2, 5, 4, 2, 1, 3, 0, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 240/240 [00:19<00:00, 12.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.898 | Train Accuracy:  0.160 | Val Loss:  0.896 | Val Accuracy:  0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 240/240 [00:19<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.896 | Train Accuracy:  0.169 | Val Loss:  0.896 | Val Accuracy:  0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 240/240 [00:19<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.896 | Train Accuracy:  0.169 | Val Loss:  0.896 | Val Accuracy:  0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 240/240 [00:20<00:00, 11.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.896 | Train Accuracy:  0.169 | Val Loss:  0.896 | Val Accuracy:  0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 240/240 [00:20<00:00, 11.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.896 | Train Accuracy:  0.169 | Val Loss:  0.896 | Val Accuracy:  0.133\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 2, 1, 1, 4, 5, 4, 3, 3, 2, 0, 4, 0, 0, 0, 1, 0, 0, 4, 0, 1, 3, 1, 1, 1, 1, 1, 4, 1, 0, 1, 1, 4, 2, 5, 3, 3, 5, 4, 0, 2, 3, 3, 5, 5, 5, 5, 2, 4, 0, 3, 3, 4, 0, 3, 5, 2, 4, 2]\n",
      "Test Accuracy:  0.183\n"
     ]
    }
   ],
   "source": [
    "evaluate(model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plot confusion matrix function\n",
    "def plot_confusion_matrix(matrix, labels):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(matrix)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + labels, rotation=90)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 2, 1, 1, 4, 5, 4, 3, 3, 2, 0, 4, 0, 0, 0, 1, 0, 0, 4, 0, 1, 3, 1, 1, 1, 1, 1, 4, 1, 0, 1, 1, 4, 2, 5, 3, 3, 5, 4, 0, 2, 3, 3, 5, 5, 5, 5, 2, 4, 0, 3, 3, 4, 0, 3, 5, 2, 4, 2]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_751/2760059089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0minput_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "test = Dataset(df_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "#model = model.cuda()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.inference_mode():            \n",
    "        for test_input, test_label in test_dataloader:\n",
    "    \n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "    \n",
    "            output = model(input_id, mask)\n",
    "            print(output)\n",
    "            y_pred.extend(output.argmax(dim=1).tolist())\n",
    "            y_true.extend(test_label.tolist())\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Austen, Jane'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''      It can be imagined that my close intimacy with Sherlock Holmes\n",
    "      had interested me deeply in crime, and that after his\n",
    "      disappearance I never failed to read with care the various\n",
    "      problems which came before the public. And I even attempted, more\n",
    "      than once, for my own private satisfaction, to employ his methods\n",
    "      in their solution, though with indifferent success. There was\n",
    "      none, however, which appealed to me like this tragedy of Ronald\n",
    "      Adair. As I read the evidence at the inquest, which led up to a\n",
    "      verdict of willful murder against some person or persons unknown,\n",
    "      I realized more clearly than I had ever done the loss which the\n",
    "      community had sustained by the death of Sherlock Holmes. There\n",
    "      were points about this strange business which would, I was sure,\n",
    "      have specially appealed to him, and the efforts of the police\n",
    "      would have been supplemented, or more probably anticipated, by\n",
    "      the trained observation and the alert mind of the first criminal\n",
    "      agent in Europe. All day, as I drove upon my round, I turned over\n",
    "      the case in my mind and found no explanation which appeared to me\n",
    "      to be adequate. At the risk of telling a twice-told tale, I will\n",
    "      recapitulate the facts as they were known to the public at the\n",
    "      conclusion of the inquest.'''\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "input_id = tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")['input_ids']\n",
    "mask = tokenizer(text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")['attention_mask']\n",
    "\n",
    "#to cuda\n",
    "input_id = input_id.cuda()\n",
    "mask = mask.cuda()\n",
    "\n",
    "output = model.forward(input_id, mask)\n",
    "\n",
    "idx2author[output.argmax(dim=1).item()]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
